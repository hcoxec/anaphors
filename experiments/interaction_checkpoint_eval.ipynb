{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2ebec0-e58c-45ac-bddd-421fbe9508df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions import Categorical\n",
    "import csv\n",
    "import egg.core as core\n",
    "from string import ascii_lowercase, punctuation, digits, ascii_uppercase\n",
    "from egg.core.util import load_interactions\n",
    "import collections\n",
    "from typing import Any, Dict, Iterable, List, NamedTuple, Optional, Union\n",
    "import json\n",
    "import _jsonnet\n",
    "from egg.zoo.channel.archs import Receiver, Sender\n",
    "from reconstructionloss import ReconstructionLoss\n",
    "from scoring import Scorer\n",
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db3b6ec-9ced-42de-981b-e602b3f1d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.load(\"datafinal/redlarge_train_set.tar\")\n",
    "training = trainset.tensors[0]\n",
    "testset = torch.load(\"datafinal/redlarge_test_set.tar\")\n",
    "testing = testset.tensors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b7887-4c61-4ed3-8d4d-92d9069fce60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    '''\n",
    "    An object that makes a dictionary's keys attributes of the object, so they can\n",
    "    be called by subscripting (mimics the functionality of argparse)\n",
    "    '''\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "args = objectview(json.loads(_jsonnet.evaluate_file('interaction_config.jsonnet')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da157efe-0481-449e-8f62-1f54e6671095",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = args.signal_chars\n",
    "embedding_size = args.embedding_size\n",
    "hidden_size = args.hidden_size\n",
    "cell_type = args.rnn_cell\n",
    "signal_len = args.signal_len-1\n",
    "\n",
    "lr = args.learning_rate\n",
    "sender_entropy = args.sender_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce786c8-c8a0-4ca7-8a81-61452e1c70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"dicts/redlarge_dict.json\") as infile:\n",
    "    grammar = json.load(infile)\n",
    "    \n",
    "initial_chars = ascii_lowercase + punctuation + digits\n",
    "msg_chars = 'E'  # to mark EOS\n",
    "msg_chars += initial_chars[:vocab_size-1]    \n",
    "\n",
    "sender = Sender(n_features=160, n_hidden=hidden_size)\n",
    "\n",
    "sender = core.RnnSenderReinforce(\n",
    "    sender,\n",
    "    vocab_size,\n",
    "    embedding_size,\n",
    "    hidden_size,\n",
    "    cell='gru',\n",
    "    max_len=signal_len,\n",
    "    num_layers=1,\n",
    "    )\n",
    "\n",
    "receiver = Receiver(n_features=160, n_hidden=hidden_size)\n",
    "receiver = core.RnnReceiverDeterministic(\n",
    "    receiver,\n",
    "    vocab_size,\n",
    "    embedding_size,\n",
    "    hidden_size,\n",
    "    cell='gru',\n",
    "    num_layers=1,\n",
    "    )\n",
    "\n",
    "loss = ReconstructionLoss(5, 32)\n",
    "game = core.SenderReceiverRnnReinforce(\n",
    "        sender,\n",
    "        receiver,\n",
    "        loss,\n",
    "        sender_entropy_coeff=sender_entropy,\n",
    "        receiver_entropy_coeff=0.0,\n",
    "        length_cost=0.15,\n",
    "        #beta1 = 45,\n",
    "        #beta2 = 1.25\n",
    "        )\n",
    "optimizer = torch.optim.Adam(game.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30bfb2a-5c0b-4091-920b-80a3cc586e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint[1])\n",
    "    sender = model.sender\n",
    "    receiver = model.receiver\n",
    "    optimizer.load_state_dict(checkpoint[2])\n",
    "    return model, sender, receiver, optimizer, checkpoint[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'interaction_final_checkpoints/properredlargefifteencost/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c794343c",
   "metadata": {},
   "source": [
    "Compute metrics for:\n",
    "\n",
    "a. Communication Accuracy\n",
    "\n",
    "b. Message Length\n",
    "\n",
    "c. Signal Uniqueness (using Jaccard Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b542d89-d164-4d90-9c02-2cbbf48c24e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "lens = []\n",
    "jaccs = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        if \".DS_Store\" in f:\n",
    "            pass\n",
    "        else:\n",
    "            print(f)\n",
    "\n",
    "            game, sender, receiver, optimizer, epoch = load_ckp(f, game, optimizer)\n",
    "\n",
    "            m = re.search(r'rs([0-9]+)', f)\n",
    "            print(m.group(1))\n",
    "                \n",
    "            scorer_train = Scorer(sender, receiver, training, game, grammar, msg_chars)\n",
    "            scorer_train.get_interactions(training, True)\n",
    "\n",
    "            lens.append(scorer_train.msg_len())\n",
    "            \n",
    "            acc, acc_or = scorer_train.dump()\n",
    "            accs.append((acc, acc_or))\n",
    "            jaccs.append([(scorer_train.uni_and_jaccard, scorer_train.uni_nonred_jaccard), (scorer_train.bi_and_jaccard, scorer_train.bi_nonred_jaccard), (scorer_train.tri_and_jaccard, scorer_train.tri_nonred_jaccard)])\n",
    "                "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01950bca",
   "metadata": {},
   "source": [
    "Get Mean Full and Partial Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean full accuracy: {np.mean(acc)}\")\n",
    "print(f\"Mean partial accuracy: {np.mean(acc_or)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "072aef82",
   "metadata": {},
   "source": [
    "Get Mean Message Lengths:\n",
    "1. All messages\n",
    "2. Partially redundant\n",
    "3. Fully redundant\n",
    "4. All redundant\n",
    "5. Non-redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78411c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All messages: {np.mean([len[0] for len in lens])}\\n\")\n",
    "print(f\"Partially redundant messages: {np.mean([len[1] for len in lens])}\\n\")\n",
    "print(f\"Fully redundant messages: {np.mean([len[2] for len in lens])}\\n\")\n",
    "print(f\"All redundant messages: {np.mean([len[3] for len in lens])}\\n\")\n",
    "print(f\"All non-redundant messages: {np.mean([len[4] for len in lens])}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a829093b",
   "metadata": {},
   "source": [
    "Get Mean Signal Uniqueness:\n",
    "1. Unigram\n",
    "2. Bigram\n",
    "3. Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c9149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal_uniqueness(jaccs):\n",
    "    unis = [j[0] for j in jaccs]\n",
    "    bis = [j[1] for j in jaccs]\n",
    "    tris = [j[2] for j in jaccs]\n",
    "    uni_diffs = [(j[1]-j[0]) for j in unis]\n",
    "    bi_diffs = [(j[1]-j[0]) for j in bis]\n",
    "    tri_diffs = [(j[1]-j[0]) for j in tris]\n",
    "    return np.mean(uni_diffs), np.mean(bi_diffs), np.mean(tri_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ddb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_signal_uniqueness(jaccs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "380c2813",
   "metadata": {},
   "source": [
    "Compute Predictive Ambiguity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70645ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recons = []\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        if \".DS_Store\" in f:\n",
    "            pass\n",
    "        else:\n",
    "            print(f)\n",
    "\n",
    "            data = []\n",
    "\n",
    "            game, sender, receiver, optimizer, epoch = load_ckp(f, game, optimizer)\n",
    "\n",
    "            m = re.search(r'rs([0-9]+)', f)\n",
    "\n",
    "            scorer_train = Scorer(sender, receiver, training, game, grammar, msg_chars)\n",
    "            scorer_train.get_interactions(training, True)\n",
    "\n",
    "            all_mean_entrops = []\n",
    "            for i in range(len(scorer_train.reconent[0])):\n",
    "                all_mean_entrops.append(scorer_train.reconent[:,i].mean().item())\n",
    "            semirednoun_mean_entrops = []\n",
    "            for i in range(len(scorer_train.semirednoun_reconent[0])):\n",
    "                semirednoun_mean_entrops.append(scorer_train.semirednoun_reconent[:,i].mean().item())\n",
    "            semiredverb_mean_entrops = []\n",
    "            for i in range(len(scorer_train.semiredverb_reconent[0])):\n",
    "                semiredverb_mean_entrops.append(scorer_train.semiredverb_reconent[:,i].mean().item())\n",
    "            fullred_mean_entrops = []\n",
    "            for i in range(len(scorer_train.red_reconent[0])):\n",
    "                fullred_mean_entrops.append(scorer_train.red_reconent[:,i].mean().item())\n",
    "            allred_mean_entrops = []\n",
    "            for i in range(len(scorer_train.allred_reconent[0])):\n",
    "                allred_mean_entrops.append(scorer_train.allred_reconent[:,i].mean().item())\n",
    "            nonred_mean_entrops = []\n",
    "            for i in range(len(scorer_train.other_reconent[0])):\n",
    "                nonred_mean_entrops.append(scorer_train.other_reconent[:,i].mean().item())\n",
    "            all_recons.append([all_mean_entrops, semirednoun_mean_entrops, semiredverb_mean_entrops, fullred_mean_entrops, allred_mean_entrops, nonred_mean_entrops])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66bf607a",
   "metadata": {},
   "source": [
    "Get Mean Predictive Ambiguity Values for each message type:\n",
    "1. All Messages\n",
    "2. Partially redundant (redundant noun)\n",
    "3. Partially redundant (redundant verb)\n",
    "4. Fully redundant\n",
    "5. All redundant\n",
    "5. Non-redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84846487",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All messages: {[np.mean([j[x] for j in [a[0] for a in all_recons]]) for x in range(len(all_recons[0][0]))]}\\n\")\n",
    "print(f\"Partially redundant messages (redundant noun): {[np.mean([j[x] for j in [a[1] for a in all_recons]]) for x in range(len(all_recons[0][0]))]}\\n\")\n",
    "print(f\"Partially redundant messages (redundant verb): {[np.mean([j[x] for j in [a[2] for a in all_recons]]) for x in range(len(all_recons[0][0]))]}\\n\")\n",
    "print(f\"Fully redundant messages: {[np.mean([j[x] for j in [a[3] for a in all_recons]]) for x in range(len(all_recons[0][0]))]}\\n\")\n",
    "print(f\"All redundant messages: {[np.mean([j[x] for j in [a[4] for a in all_recons]]) for x in range(len(all_recons[0][0]))]}\\n\")\n",
    "print(f\"All non-redundant messages: {[np.mean([j[x] for j in [a[5] for a in all_recons]]) for x in range(len(all_recons[0][0]))]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
