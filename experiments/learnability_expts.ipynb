{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import csv\n",
    "import egg.core as core\n",
    "from discourse_lang import DataHandler\n",
    "from string import ascii_lowercase, punctuation, digits\n",
    "import json\n",
    "import _jsonnet\n",
    "from egg.zoo.channel.archs import Receiver, Sender\n",
    "from reconstructionloss import ReconstructionLoss\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N.B. Languages are referred to in code as follows:\n",
    "\n",
    "No Elision --> Comp\n",
    "\n",
    "Pronoun --> Tok\n",
    "\n",
    "Pro-drop --> Null"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change language used, change dataset, gram_fn and setting in learnability_config.jsonnet\n",
    "\n",
    "Options are:\n",
    "- No Elision: redlarge_comp\n",
    "- Pronoun: redlarge_tok\n",
    "- Pro-drop: redlarge_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    '''\n",
    "    An object that makes a dictionary's keys attributes of the object, so they can\n",
    "    be called by subscripting (mimics the functionality of argparse)\n",
    "    '''\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "\n",
    "args = objectview(json.loads(_jsonnet.evaluate_file('learnability_config.jsonnet')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataHandler(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = args.signal_chars #for comp and null languages\n",
    "#vocab_size = args.signal_chars+2 #for tok language\n",
    "embedding_size = args.embedding_size\n",
    "hidden_size = args.hidden_size\n",
    "cell_type = args.rnn_cell\n",
    "signal_len = args.signal_len-1\n",
    "\n",
    "lr = args.learning_rate\n",
    "sender_entropy = args.sender_entropy\n",
    "gram_fn = args.gram_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"dicts/{gram_fn}_dict.json\") as infile:\n",
    "    grammar = json.load(infile)\n",
    "    \n",
    "initial_chars = ascii_lowercase + punctuation + digits\n",
    "msg_chars = 'E'  # to mark EOS\n",
    "msg_chars += initial_chars[:vocab_size-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint[1])\n",
    "    sender = model.sender\n",
    "    receiver = model.receiver\n",
    "    optimizer.load_state_dict(checkpoint[2])\n",
    "    return model, sender, receiver, optimizer, checkpoint[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_train(model, optimizer, train_dataset, val_dataset, epochs, run):\n",
    "    \n",
    "    train_checkpoints = []\n",
    "    val_checkpoints = []\n",
    "    \n",
    "    model.return_raw = True\n",
    "    final_break = False\n",
    "    #model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        if final_break:\n",
    "            break\n",
    "        \n",
    "        mean_loss, n_batches = 0, 0\n",
    "        for data, target in train_dataset:\n",
    "                    \n",
    "            #data, target = data.to(device), target.to(device)  \n",
    "            optimizer.zero_grad()\n",
    "                        \n",
    "            output = model(data)\n",
    "            \n",
    "            oo = output[0].view(output[0].shape[0]*5, 32)\n",
    "            \n",
    "            target = target.view(target.shape[0], 5, 32)\n",
    "            target = target.argmax(dim=-1).view(target.shape[0]*5)\n",
    "            \n",
    "            loss = F.cross_entropy(oo, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            mean_loss += loss.mean().item()\n",
    "            n_batches += 1\n",
    "        \n",
    "        train_checkpoints.append((epoch, mean_loss/n_batches))\n",
    "        print(f'Train Epoch: {epoch}, mean loss: {mean_loss / n_batches}')\n",
    "\n",
    "\n",
    "        #Validation\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            \n",
    "            vmean_loss, vn_batches = 0, 0\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for data, target in val_dataset:\n",
    "            \n",
    "                    #data, target = data.to(device), target.to(device)  \n",
    "\n",
    "                    output = model(data)\n",
    "\n",
    "                    oo = output[0].view(output[0].shape[0]*5, 32)\n",
    "\n",
    "                    target = target.view(target.shape[0], 5, 32)\n",
    "                    target = target.argmax(dim=-1).view(target.shape[0]*5)\n",
    "\n",
    "                    loss = F.cross_entropy(oo, target)\n",
    "\n",
    "                    vmean_loss += loss.mean().item()\n",
    "                    vn_batches += 1\n",
    "        \n",
    "                val_checkpoints.append((epoch, vmean_loss/vn_batches))\n",
    "                print(f'Val Epoch: {epoch}, val mean loss: {vmean_loss / vn_batches}')\n",
    "                torch.save(receiver.state_dict(), f\"receiver_state_dicts_v2/comp/{run}/{epoch}.tar\")\n",
    "             \n",
    "            model.train()\n",
    "                \n",
    "    return train_checkpoints, val_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = Sender(n_features=160, n_hidden=hidden_size)\n",
    "\n",
    "sender = core.RnnSenderReinforce(\n",
    "    sender,\n",
    "    vocab_size,\n",
    "    embedding_size,\n",
    "    hidden_size,\n",
    "    cell='gru',\n",
    "    max_len=signal_len,\n",
    "    num_layers=1,\n",
    "    )\n",
    "\n",
    "receiver = Receiver(n_features=160, n_hidden=hidden_size)\n",
    "receiver = core.RnnReceiverDeterministic(\n",
    "    receiver,\n",
    "    vocab_size,\n",
    "    embedding_size,\n",
    "    hidden_size,\n",
    "    cell='gru',\n",
    "    num_layers=1,\n",
    "    )\n",
    "\n",
    "loss = ReconstructionLoss(5, 32)\n",
    "game = core.SenderReceiverRnnReinforce(\n",
    "        sender,\n",
    "        receiver,\n",
    "        loss,\n",
    "        sender_entropy_coeff=sender_entropy,\n",
    "        receiver_entropy_coeff=0.0,\n",
    "        length_cost=0.0,\n",
    "        )\n",
    "optimizer = torch.optim.Adam(game.parameters(), lr=lr)\n",
    "\n",
    "training_loader = data.train_comp_loader\n",
    "val_loader = data.val_comp_loader\n",
    "test_loader = data.test_comp_loader\n",
    "\n",
    "optimizer = torch.optim.Adam(receiver.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = []\n",
    "for n in range(10):\n",
    "    sender = Sender(n_features=160, n_hidden=hidden_size)\n",
    "\n",
    "    sender = core.RnnSenderReinforce(\n",
    "        sender,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        hidden_size,\n",
    "        cell='gru',\n",
    "        max_len=signal_len,\n",
    "        num_layers=1,\n",
    "        )\n",
    "\n",
    "    receiver = Receiver(n_features=160, n_hidden=hidden_size)\n",
    "    receiver = core.RnnReceiverDeterministic(\n",
    "        receiver,\n",
    "        vocab_size,\n",
    "        embedding_size,\n",
    "        hidden_size,\n",
    "        cell='gru',\n",
    "        num_layers=1,\n",
    "        )\n",
    "\n",
    "    loss = ReconstructionLoss(5, 32)\n",
    "    game = core.SenderReceiverRnnReinforce(\n",
    "            sender,\n",
    "            receiver,\n",
    "            loss,\n",
    "            sender_entropy_coeff=sender_entropy,\n",
    "            receiver_entropy_coeff=0.0,\n",
    "            length_cost=0.0,\n",
    "            )\n",
    "    optimizer = torch.optim.Adam(game.parameters(), lr=lr)\n",
    "    \n",
    "    training_loader = data.train_comp_loader\n",
    "    val_loader = data.val_comp_loader\n",
    "    test_loader = data.test_comp_loader\n",
    "\n",
    "    optimizer = torch.optim.Adam(receiver.parameters(), lr=lr)\n",
    "    \n",
    "    train_checkpoints, val_checkpoints = quick_train(receiver, optimizer, training_loader, val_loader, epochs=100, run=n)\n",
    "    checkpoints.append({str(n): [train_checkpoints, val_checkpoints]})\n",
    "\n",
    "with open(\"learnability_final_checkpoints/receiver_comp_checkpoints\", 'w') as outf:\n",
    "    json.dump(checkpoints, outf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, path): \n",
    "    # Load the model that we saved at the end of the training loop \n",
    "    model = receiver \n",
    "    model.load_state_dict(torch.load(path)) \n",
    "    model.eval()\n",
    "    #running_accuracy = 0\n",
    "    exact_accuracy = 0\n",
    "    partial_accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader: \n",
    "            \n",
    "            inputs, outputs = data, target\n",
    "            \n",
    "            outputs = outputs.to(torch.float32)\n",
    "            outputs = outputs.view(outputs.shape[0], 5, 32)\n",
    "            outputs = outputs.argmax(dim=-1)\n",
    "\n",
    "            predicted_outputs = model(inputs)[0]\n",
    "            predicted_outputs = predicted_outputs.view(predicted_outputs.shape[0], 5, 32)\n",
    "            predicted = predicted_outputs.argmax(dim=-1)\n",
    "            \n",
    "            exact = (torch.sum((predicted == outputs).detach(), dim=1) == 5).sum().item()\n",
    "            partial = (predicted == outputs).sum().item()\n",
    "\n",
    "            total += outputs.size(0)\n",
    "\n",
    "            exact_accuracy += exact\n",
    "            partial_accuracy += partial\n",
    "        \n",
    "        #print(total)\n",
    "        print(f\"exact: {exact_accuracy/total}, partial: {partial_accuracy/(total*5)}\")\n",
    "        return ({exact_accuracy/total}, {partial_accuracy/(total*5)})\n",
    "        \n",
    "        #print('inputs is: %d %%' % (100 * running_accuracy / total))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy Calculation (compute earliest epoch for 100% accuracy)\n",
    "- This may take a minute or two to process for each run (loading in and testing all the checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "directory = \"learnability_state_dicts/comp/1/\"\n",
    "res = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    if \".DS_Store\" in f:\n",
    "        pass\n",
    "    else:\n",
    "        #print(f)\n",
    "        epoch = re.match(r'.*\\/(\\d{1,2}).tar', f).group(1)\n",
    "        #print(f)\n",
    "        res.append((int(epoch), [test(training_loader, f), test(val_loader, f), test(test_loader, f)]))\n",
    "#         test(training_loader, f)\n",
    "#         test(val_loader, f)\n",
    "#         test(test_loader, f)\n",
    "#         print()\n",
    "#sorted(res, key = lambda x: x[0])\n",
    "\n",
    "sres = sorted(res, key = lambda x: x[0])\n",
    "\n",
    "test_accs = [(x[0], list(x[1][2][1])[0]) for x in sres]\n",
    "for x in test_accs:\n",
    "    if x[1] == 1.0:\n",
    "        print()\n",
    "        print(\"Earliest Epoch: \", x[0])\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Ambiguity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiver = Receiver(n_features=160, n_hidden=hidden_size)\n",
    "receiver = core.RnnReceiverDeterministic(\n",
    "    receiver,\n",
    "    vocab_size,\n",
    "    embedding_size,\n",
    "    hidden_size,\n",
    "    cell='gru',\n",
    "    num_layers=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = receiver\n",
    "model.load_state_dict(torch.load(\"receiver_state_dicts_v2/comp/9/25.tar\"))\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data.comp_train_set\n",
    "val_set = data.comp_val_set\n",
    "test_set = data.comp_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "all_msgs = []\n",
    "red_msgs = []\n",
    "#semired_msgs = []\n",
    "semired_noun_msgs = []\n",
    "semired_verb_msgs = []\n",
    "fullred_msgs = []\n",
    "other_msgs = []\n",
    "\n",
    "for msg, dat in train_set:\n",
    "    all_msgs.append(msg)\n",
    "    #if len(msg[msg.nonzero().squeeze().detach()]) < 10:\n",
    "    dat = dat.view(5, 32)\n",
    "    if torch.equal(dat[0], dat[3]) or torch.equal(dat[1], dat[4]):\n",
    "        red_msgs.append(msg)\n",
    "        #if len(msg[msg.nonzero().squeeze().detach()]) == 8:\n",
    "        if torch.equal(dat[0], dat[3]) and torch.equal(dat[1], dat[4]):\n",
    "            fullred_msgs.append(msg)\n",
    "        elif torch.equal(dat[0], dat[3]):\n",
    "            semired_noun_msgs.append(msg)\n",
    "            #semired_msgs.append(msg)\n",
    "        elif torch.equal(dat[1], dat[4]):\n",
    "            semired_verb_msgs.append(msg)\n",
    "#             if torch.equal(dat[0], dat[3]):\n",
    "#                 semired_noun_msgs.append(msg)\n",
    "#             elif torch.equal(dat[1], dat[4]):\n",
    "#                 semired_verb_msgs.append(msg)\n",
    "        #elif len(msg[msg.nonzero().squeeze().detach()]) < 8:\n",
    "        #    fullred_msgs.append(msg)\n",
    "    else:\n",
    "        other_msgs.append(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signals = torch.stack(all_msgs)\n",
    "red_signals = torch.stack(red_msgs)\n",
    "#semired_signals = torch.stack(semired_msgs)\n",
    "semired_noun_signals = torch.stack(semired_noun_msgs)\n",
    "semired_verb_signals = torch.stack(semired_verb_msgs)\n",
    "fullred_signals = torch.stack(fullred_msgs)\n",
    "other_signals = torch.stack(other_msgs)\n",
    "\n",
    "all_outputs = model(all_signals)[0]\n",
    "red_outputs = model(red_signals)[0]\n",
    "#semired_outputs = receiver(semired_signals)[0]\n",
    "semired_noun_outputs = model(semired_noun_signals)[0]\n",
    "semired_verb_outputs = model(semired_verb_signals)[0]\n",
    "fullred_outputs = model(fullred_signals)[0]\n",
    "other_outputs = model(other_signals)[0]\n",
    "\n",
    "all_reconents = torch.distributions.Categorical(logits = all_outputs.view(len(all_outputs), 5, 32)).entropy()\n",
    "red_reconents = torch.distributions.Categorical(logits = red_outputs.view(len(red_outputs), 5, 32)).entropy()\n",
    "#semired_reconents = torch.distributions.Categorical(logits = semired_outputs.view(len(semired_outputs), 5, 32)).entropy()\n",
    "semired_noun_reconents = torch.distributions.Categorical(logits = semired_noun_outputs.view(len(semired_noun_outputs), 5, 32)).entropy()\n",
    "semired_verb_reconents = torch.distributions.Categorical(logits = semired_verb_outputs.view(len(semired_verb_outputs), 5, 32)).entropy()\n",
    "fullred_reconents = torch.distributions.Categorical(logits = fullred_outputs.view(len(fullred_outputs), 5, 32)).entropy()\n",
    "other_reconents = torch.distributions.Categorical(logits = other_outputs.view(len(other_outputs), 5, 32)).entropy()\n",
    "\n",
    "mean_all_entrops = []\n",
    "mean_red_entrops = []\n",
    "#mean_semired_entrops = []\n",
    "mean_semired_noun_entrops = []\n",
    "mean_semired_verb_entrops = []\n",
    "mean_fullred_entrops = []\n",
    "mean_other_entrops = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write values to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"receivercompreconents_v2.csv\", 'w') as inf:\n",
    "    writer = csv.writer(inf)\n",
    "    \n",
    "    header = [\"train.reconent1\", \"train.reconent2\", \"train.reconent3\", \"train.reconent4\", \"train.reconent5\", \"train.semirednoun_reconent1\", \"train.semirednoun_reconent2\", \"train.semirednoun_reconent3\", \"train.semirednoun_reconent4\", \"train.semirednoun_reconent5\", \"train.semiredverb_reconent1\", \"train.semiredverb_reconent2\", \"train.semiredverb_reconent3\", \"train.semiredverb_reconent4\", \"train.semiredverb_reconent5\", \"train.red_reconent1\", \"train.red_reconent2\", \"train.red_reconent3\", \"train.red_reconent4\", \"train.red_reconent5\", \"train.allred_reconent1\", \"train.allred_reconent2\", \"train.allred_reconent3\", \"train.allred_reconent4\", \"train.allred_reconent5\", \"train.other_reconent1\", \"train.other_reconent2\", \"train.other_reconent3\", \"train.other_reconent4\", \"train.other_reconent5\"]\n",
    "    \n",
    "    #header = [\"test.reconent1\", \"test.reconent2\", \"test.reconent3\", \"test.reconent4\", \"test.reconent5\", \"test.semirednoun1_reconent\", \"test.semirednoun2_reconent\", \"test.semirednoun3_reconent\", \"test.semirednoun4_reconent\", \"test.semirednoun5_reconent\", \"test.semiredverb1_reconent\", \"test.semiredverb2_reconent\", \"test.semiredverb3_reconent\", \"test.semiredverb4_reconent\", \"test.semiredverb5_reconent\", \"test.red_reconent1\", \"test.red_reconent2\", \"test.red_reconent3\", \"test.red_reconent4\", \"test.red_reconent5\", \"test.allred_reconent1\", \"test.allred_reconent2\", \"test.allred_reconent3\", \"test.allred_reconent4\", \"test.allred_reconent5\", \"test.other_reconent1\", \"test.other_reconent2\", \"test.other_reconent3\", \"test.other_reconent4\", \"test.other_reconent5\"]\n",
    "    \n",
    "    writer.writerow(header)\n",
    "    \n",
    "    csv_data = []\n",
    "\n",
    "    for i in range(len(all_reconents[0])):\n",
    "        mean_all_entrops.append(all_reconents[:,i].mean().item())\n",
    "        \n",
    "    csv_data.append(mean_all_entrops[0])\n",
    "    csv_data.append(mean_all_entrops[1])\n",
    "    csv_data.append(mean_all_entrops[2])\n",
    "    csv_data.append(mean_all_entrops[3])\n",
    "    csv_data.append(mean_all_entrops[4])\n",
    "    \n",
    "    for i in range(len(semired_noun_reconents[0])):\n",
    "        mean_semired_noun_entrops.append(semired_noun_reconents[:,i].mean().item())\n",
    "    \n",
    "    csv_data.append(mean_semired_noun_entrops[0])\n",
    "    csv_data.append(mean_semired_noun_entrops[1])\n",
    "    csv_data.append(mean_semired_noun_entrops[2])\n",
    "    csv_data.append(mean_semired_noun_entrops[3])\n",
    "    csv_data.append(mean_semired_noun_entrops[4])\n",
    "    \n",
    "    for i in range(len(semired_verb_reconents[0])):\n",
    "        mean_semired_verb_entrops.append(semired_verb_reconents[:,i].mean().item())\n",
    "    \n",
    "    csv_data.append(mean_semired_verb_entrops[0])\n",
    "    csv_data.append(mean_semired_verb_entrops[1])\n",
    "    csv_data.append(mean_semired_verb_entrops[2])\n",
    "    csv_data.append(mean_semired_verb_entrops[3])\n",
    "    csv_data.append(mean_semired_verb_entrops[4])\n",
    "    \n",
    "    for i in range(len(fullred_reconents[0])):\n",
    "        mean_fullred_entrops.append(fullred_reconents[:,i].mean().item())\n",
    "        \n",
    "    csv_data.append(mean_fullred_entrops[0])\n",
    "    csv_data.append(mean_fullred_entrops[1])\n",
    "    csv_data.append(mean_fullred_entrops[2])\n",
    "    csv_data.append(mean_fullred_entrops[3])\n",
    "    csv_data.append(mean_fullred_entrops[4])\n",
    "    \n",
    "    for i in range(len(red_reconents[0])):\n",
    "        mean_red_entrops.append(red_reconents[:,i].mean().item())\n",
    "    \n",
    "    csv_data.append(mean_red_entrops[0])\n",
    "    csv_data.append(mean_red_entrops[1])\n",
    "    csv_data.append(mean_red_entrops[2])\n",
    "    csv_data.append(mean_red_entrops[3])\n",
    "    csv_data.append(mean_red_entrops[4])\n",
    "    \n",
    "    for i in range(len(other_reconents[0])):\n",
    "        mean_other_entrops.append(other_reconents[:,i].mean().item())\n",
    "    \n",
    "    csv_data.append(mean_other_entrops[0])\n",
    "    csv_data.append(mean_other_entrops[1])\n",
    "    csv_data.append(mean_other_entrops[2])\n",
    "    csv_data.append(mean_other_entrops[3])\n",
    "    csv_data.append(mean_other_entrops[4])\n",
    "    \n",
    "    writer.writerow(csv_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a01bc1298d69bd7ded960bb20c5eaf90fc263047ce97122f454fafe293b105c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
